{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import timeit as tm\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "## Loading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data dimensions: (295169L, 64L)\n",
      "training label dimensions: (295169L, 6L)\n"
     ]
    }
   ],
   "source": [
    "# Open training data to pandas\n",
    "train_dat_pandas = pd.read_csv('../data/clean_data/train_vectors.csv', index_col=0, encoding='utf-8')\n",
    "del train_dat_pandas['TYPE']\n",
    "\n",
    "# Open training labels to pandas\n",
    "train_lbl_pandas = pd.read_csv('../data/clean_data/train_labels.csv', index_col=0, encoding='utf-8')\n",
    "del train_lbl_pandas['YEAR']\n",
    "\n",
    "# Save headers\n",
    "headers = [list(train_dat_pandas)]\n",
    "\n",
    "# Convert pandas to numpy matrix\n",
    "train_dat = train_dat_pandas.as_matrix()\n",
    "print 'training data dimensions:', train_dat.shape\n",
    "\n",
    "# Convert pandas to numpy matrix\n",
    "train_lbl = train_lbl_pandas.as_matrix()\n",
    "print 'training label dimensions:', train_lbl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing data dimensions: (34142L, 64L)\n",
      "testing label dimensions: (34142L, 6L)\n"
     ]
    }
   ],
   "source": [
    "# Open test data\n",
    "test_dat_pandas = pd.read_csv('../data/clean_data/test_vectors.csv', index_col=0, encoding='utf-8')\n",
    "del test_dat_pandas['TYPE']\n",
    "\n",
    "# Open test labels\n",
    "test_lbl_pandas = pd.read_csv('../data/clean_data/test_labels.csv', index_col=0, encoding='utf-8')\n",
    "del test_lbl_pandas['YEAR']\n",
    "\n",
    "# Convert pandas to numpy matrix\n",
    "test_dat = test_dat_pandas.as_matrix()\n",
    "print 'testing data dimensions:', test_dat.shape\n",
    "\n",
    "# Convert pandas to numpy matrix\n",
    "test_lbl = test_lbl_pandas.as_matrix()\n",
    "print 'testing label dimensions:', test_lbl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting one hot labels to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 ..., 4 2 3] [4 2 2 ..., 4 2 2]\n"
     ]
    }
   ],
   "source": [
    "# method to convert a one hot encoding array into a numeric array\n",
    "def onehot_2_numeric(onehot):\n",
    "    numeric = []\n",
    "    for elem in onehot:\n",
    "        result = 0\n",
    "        for i, k in enumerate(elem):\n",
    "            result += i * k\n",
    "        numeric.append(result)\n",
    "    return np.asarray(numeric)\n",
    "\n",
    "\n",
    "train_lbl_txt = onehot_2_numeric(train_lbl)\n",
    "test_lbl_txt = onehot_2_numeric(test_lbl)\n",
    "print train_lbl_txt, test_lbl_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature vector scalling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_dat)\n",
    "train_dat = scaler.transform(train_dat)\n",
    "test_dat = scaler.transform(test_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit Linear Regression\n",
    "lin_reg = LinearRegression(n_jobs=-1, normalize=True)\n",
    "lin_reg.fit(train_dat, train_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34142L, 6L)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predictions = lin_reg.predict(test_dat)\n",
    "print predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error: 0.841279191841\n"
     ]
    }
   ],
   "source": [
    "# Compute RMSE\n",
    "\n",
    "import math\n",
    "\n",
    "errors = []\n",
    "\n",
    "# compute squared errors\n",
    "for i in xrange(predictions.shape[0]):\n",
    "    p = predictions[i]\n",
    "    t = test_lbl[i]\n",
    "    \n",
    "    # compute distance\n",
    "    squared_distance = 0.0\n",
    "    for j in xrange(predictions.shape[1]):\n",
    "        squared_distance += (p[j] - t[j])**2\n",
    "    \n",
    "    errors.append(squared_distance)\n",
    "\n",
    "rmse = math.sqrt(sum(errors)/len(errors))\n",
    "print 'Root mean squared error:', rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/linear_regression_model.p',\n",
       " '../models/linear_regression_model.p_01.npy',\n",
       " '../models/linear_regression_model.p_02.npy',\n",
       " '../models/linear_regression_model.p_03.npy',\n",
       " '../models/linear_regression_model.p_04.npy']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(lin_reg, '../models/linear_regression_model.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# clf = LogisticRegression(n_jobs=-1)\n",
    "# clf.fit(train_dat, train_lbl_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predictions = clf.predict(test_dat)\n",
    "# p_predictions = clf.predict_proba(test_dat)\n",
    "\n",
    "# print 'predictions dimensions:', predictions.shape\n",
    "# print 'probabilities per class:', p_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Table of probabilities for each class\n",
    "# for i in range(6):\n",
    "#     print str(i)+'\\t',\n",
    "\n",
    "# print ''\n",
    "\n",
    "# for i in xrange(len(p_predictions)):\n",
    "    \n",
    "#     for j in xrange(len(p_predictions[i])):\n",
    "#         print(\"%.2f\" % (p_predictions[i][j]*100))+'%\\t',\n",
    "    \n",
    "#     print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# score = accuracy_score(test_lbl_txt, predictions)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionCV(Cs=10, class_weight=None,\n",
      "           cv=sklearn.cross_validation.StratifiedKFold(labels=[4 4 ..., 2 3], n_folds=5, shuffle=False, random_state=None),\n",
      "           dual=False, fit_intercept=True, intercept_scaling=1.0,\n",
      "           max_iter=100, multi_class='ovr', n_jobs=-1, penalty='l2',\n",
      "           random_state=None, refit=True, scoring=None, solver='liblinear',\n",
      "           tol=0.0001, verbose=5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "folder = StratifiedKFold(train_lbl_txt, n_folds=5, shuffle=False)\n",
    "\n",
    "clf = LogisticRegressionCV(n_jobs=-1, solver='liblinear', cv=folder, verbose=5)\n",
    "print clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 27.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]0.425692695214\n"
     ]
    }
   ],
   "source": [
    "clf = clf.fit(train_dat, train_lbl_txt)\n",
    "print clf.score(test_dat, test_lbl_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/logistic_regression_model.p',\n",
       " '../models/logistic_regression_model.p_01.npy',\n",
       " '../models/logistic_regression_model.p_02.npy',\n",
       " '../models/logistic_regression_model.p_03.npy',\n",
       " '../models/logistic_regression_model.p_04.npy',\n",
       " '../models/logistic_regression_model.p_05.npy',\n",
       " '../models/logistic_regression_model.p_06.npy',\n",
       " '../models/logistic_regression_model.p_07.npy',\n",
       " '../models/logistic_regression_model.p_08.npy',\n",
       " '../models/logistic_regression_model.p_09.npy',\n",
       " '../models/logistic_regression_model.p_10.npy',\n",
       " '../models/logistic_regression_model.p_11.npy',\n",
       " '../models/logistic_regression_model.p_12.npy',\n",
       " '../models/logistic_regression_model.p_13.npy',\n",
       " '../models/logistic_regression_model.p_14.npy',\n",
       " '../models/logistic_regression_model.p_15.npy',\n",
       " '../models/logistic_regression_model.p_16.npy',\n",
       " '../models/logistic_regression_model.p_17.npy',\n",
       " '../models/logistic_regression_model.p_18.npy',\n",
       " '../models/logistic_regression_model.p_19.npy',\n",
       " '../models/logistic_regression_model.p_20.npy',\n",
       " '../models/logistic_regression_model.p_21.npy']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, '../models/logistic_regression_model.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.387089215629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(train_dat, train_lbl_txt)\n",
    "predictions = clf.predict(test_dat)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(test_lbl_txt, predictions)\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:   27.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Score on test data: 0.440044519946\n",
      "best params: {'max_depth': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "folder = StratifiedKFold(train_lbl_txt, n_folds=5, shuffle=False)\n",
    "parameters = {'max_depth':[None, 2, 4, 8, 16, 32, 64]}\n",
    "dtc_clf = DecisionTreeClassifier()\n",
    "\n",
    "clf = GridSearchCV(dtc_clf, parameters, n_jobs=-1, pre_dispatch='n_jobs', cv=folder, refit=True, verbose=5)\n",
    "clf.fit(train_dat, train_lbl_txt)\n",
    "\n",
    "print 'Score on test data:', clf.score(test_dat, test_lbl_txt)\n",
    "\n",
    "print 'best params:', clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/decision_tree_model.p',\n",
       " '../models/decision_tree_model.p_01.npy',\n",
       " '../models/decision_tree_model.p_02.npy',\n",
       " '../models/decision_tree_model.p_03.npy',\n",
       " '../models/decision_tree_model.p_04.npy',\n",
       " '../models/decision_tree_model.p_05.npy',\n",
       " '../models/decision_tree_model.p_06.npy',\n",
       " '../models/decision_tree_model.p_07.npy',\n",
       " '../models/decision_tree_model.p_08.npy',\n",
       " '../models/decision_tree_model.p_09.npy',\n",
       " '../models/decision_tree_model.p_10.npy',\n",
       " '../models/decision_tree_model.p_11.npy',\n",
       " '../models/decision_tree_model.p_12.npy',\n",
       " '../models/decision_tree_model.p_13.npy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, '../models/decision_tree_model.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this will take a while... None 0.406795738266\n",
      "None 0.404532827427\n",
      "None 0.406941879965\n",
      "None 0.404966797669\n",
      "None 0.405498805712\n",
      "2 0.387706014872\n",
      "2 0.386086455722\n",
      "2 0.389307675368\n",
      "2 0.388314812305\n",
      "2 0.388135047687\n",
      "4 0.401917441604\n",
      "4 0.399806897486\n",
      "4 0.403367607948\n",
      "4 0.403154221439\n",
      "4 0.403703139029\n",
      "8 0.433744939614\n",
      "8 0.434887187479\n",
      "8 0.431690749242\n",
      "8 0.430359804852\n",
      "8 0.430231573241\n",
      "16 0.455019733388\n",
      "16 0.455332339589\n",
      "16 0.455626513984\n",
      "16 0.455329312915\n",
      "16 0.455557249581\n",
      "32 0.408472652743\n",
      "32 0.406074259774\n",
      "32 0.408432571612\n",
      "32 0.409303428649\n",
      "32 0.408548051024\n",
      "64 0.406287582364\n",
      "64 0.404786909682\n",
      "64 0.408330933546\n",
      "64 0.40632199485\n",
      "64 0.404770374888\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate k-fold split in the training data\n",
    "kf = StratifiedKFold(train_lbl_txt, n_folds=5, shuffle=True)\n",
    "\n",
    "# Do multiple runs and save'em to runs list\n",
    "runs = []\n",
    "depths = [None, 2, 4, 8, 16, 32, 64]\n",
    "\n",
    "print 'this will take a while...',\n",
    "for d in depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=d)\n",
    "    for t,v in kf:\n",
    "        trn = train_dat[t]\n",
    "        val = train_dat[v]\n",
    "        trn_lbl = train_lbl_txt[t]\n",
    "        val_lbl = train_lbl_txt[v]\n",
    "        clf.fit(trn, trn_lbl)\n",
    "        predictions = clf.predict(val)\n",
    "        #score = accuracy_score(val_lbl, predictions)\n",
    "        score = clf.score(val, val_lbl)\n",
    "        runs.append(tuple([d, score]))\n",
    "        print d, score\n",
    "print 'done!'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best result: (16, 0.45562651398370402)\n",
      "Score on test data: 0.468513853904\n"
     ]
    }
   ],
   "source": [
    "best_result = max(runs, key=lambda run: run[1])\n",
    "print 'Best result:', best_result\n",
    "best_d = best_result[0]\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=best_d)\n",
    "clf.fit(train_dat, train_lbl_txt)\n",
    "print 'Score on test data:', clf.score(test_dat, test_lbl_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49935563235897135"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "clf.fit(train_dat, train_lbl_txt)\n",
    "clf.score(test_dat, test_lbl_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation on random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this will take a while...\n",
      "run: None auto gini done!\n",
      "run: None auto entropy done!\n",
      "run: None log2 gini done!\n",
      "run: None log2 entropy done!\n",
      "run: None None gini done!\n",
      "run: None None entropy done!\n",
      "run: 2 auto gini done!\n",
      "run: 2 auto entropy done!\n",
      "run: 2 log2 gini done!\n",
      "run: 2 log2 entropy done!\n",
      "run: 2 None gini done!\n",
      "run: 2 None entropy done!\n",
      "run: 4 auto gini done!\n",
      "run: 4 auto entropy done!\n",
      "run: 4 log2 gini done!\n",
      "run: 4 log2 entropy done!\n",
      "run: 4 None gini done!\n",
      "run: 4 None entropy done!\n",
      "run: 8 auto gini done!\n",
      "run: 8 auto entropy done!\n",
      "run: 8 log2 gini done!\n",
      "run: 8 log2 entropy done!\n",
      "run: 8 None gini done!\n",
      "run: 8 None entropy done!\n",
      "run: 16 auto gini done!\n",
      "run: 16 auto entropy done!\n",
      "run: 16 log2 gini done!\n",
      "run: 16 log2 entropy done!\n",
      "run: 16 None gini done!\n",
      "run: 16 None entropy done!\n",
      "run: 32 auto gini done!\n",
      "run: 32 auto entropy done!\n",
      "run: 32 log2 gini done!\n",
      "run: 32 log2 entropy done!\n",
      "run: 32 None gini done!\n",
      "run: 32 None entropy done!\n",
      "run: 64 auto gini done!\n",
      "run: 64 auto entropy done!\n",
      "run: 64 log2 gini done!\n",
      "run: 64 log2 entropy done!\n",
      "run: 64 None gini done!\n",
      "run: 64 None entropy done!\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# Generate k-fold split in the training data\n",
    "kf = StratifiedKFold(train_lbl_txt, n_folds=5, shuffle=True)\n",
    "\n",
    "# Do multiple runs and save'em to runs list\n",
    "runs = []\n",
    "params = []\n",
    "\n",
    "depths = [None, 2, 4, 8, 16, 32, 64]\n",
    "max_features = ['auto', 'log2', None]\n",
    "criterions = ['gini', 'entropy']\n",
    "for d in depths:\n",
    "    for mf in max_features:\n",
    "        for c in criterions:\n",
    "            params.append([d, mf, c])\n",
    "\n",
    "\n",
    "print 'this will take a while...'\n",
    "for d, mf, c in params:\n",
    "    clf = RandomForestClassifier(n_jobs=-1, max_depth=d, max_features=mf, criterion=c)\n",
    "    print 'run:', d, mf, c,\n",
    "    for t,v in kf:\n",
    "        trn = train_dat[t]\n",
    "        val = train_dat[v]\n",
    "        trn_lbl = train_lbl_txt[t]\n",
    "        val_lbl = train_lbl_txt[v]\n",
    "        clf.fit(trn, trn_lbl)\n",
    "        score = clf.score(val, val_lbl)\n",
    "        runs.append([score, d, mf, c])\n",
    "    print 'done!'\n",
    "print 'All done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/random_forest_model.p',\n",
       " '../models/random_forest_model.p_01.npy',\n",
       " '../models/random_forest_model.p_02.npy',\n",
       " '../models/random_forest_model.p_03.npy',\n",
       " '../models/random_forest_model.p_04.npy',\n",
       " '../models/random_forest_model.p_05.npy',\n",
       " '../models/random_forest_model.p_06.npy',\n",
       " '../models/random_forest_model.p_07.npy',\n",
       " '../models/random_forest_model.p_08.npy',\n",
       " '../models/random_forest_model.p_09.npy',\n",
       " '../models/random_forest_model.p_10.npy',\n",
       " '../models/random_forest_model.p_11.npy',\n",
       " '../models/random_forest_model.p_12.npy',\n",
       " '../models/random_forest_model.p_13.npy',\n",
       " '../models/random_forest_model.p_14.npy',\n",
       " '../models/random_forest_model.p_15.npy',\n",
       " '../models/random_forest_model.p_16.npy',\n",
       " '../models/random_forest_model.p_17.npy',\n",
       " '../models/random_forest_model.p_18.npy',\n",
       " '../models/random_forest_model.p_19.npy',\n",
       " '../models/random_forest_model.p_20.npy',\n",
       " '../models/random_forest_model.p_21.npy',\n",
       " '../models/random_forest_model.p_22.npy',\n",
       " '../models/random_forest_model.p_23.npy',\n",
       " '../models/random_forest_model.p_24.npy',\n",
       " '../models/random_forest_model.p_25.npy',\n",
       " '../models/random_forest_model.p_26.npy',\n",
       " '../models/random_forest_model.p_27.npy',\n",
       " '../models/random_forest_model.p_28.npy',\n",
       " '../models/random_forest_model.p_29.npy',\n",
       " '../models/random_forest_model.p_30.npy',\n",
       " '../models/random_forest_model.p_31.npy',\n",
       " '../models/random_forest_model.p_32.npy',\n",
       " '../models/random_forest_model.p_33.npy',\n",
       " '../models/random_forest_model.p_34.npy',\n",
       " '../models/random_forest_model.p_35.npy',\n",
       " '../models/random_forest_model.p_36.npy',\n",
       " '../models/random_forest_model.p_37.npy',\n",
       " '../models/random_forest_model.p_38.npy',\n",
       " '../models/random_forest_model.p_39.npy',\n",
       " '../models/random_forest_model.p_40.npy',\n",
       " '../models/random_forest_model.p_41.npy']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "champion = max(runs, key=lambda run: run[0])\n",
    "score, d, mf, c = champion\n",
    "clf = RandomForestClassifier(n_jobs=-1, max_depth=d, max_features=mf, criterion=c)\n",
    "clf.fit(train_dat, train_lbl_txt)\n",
    "clf.score(test_dat, test_lbl_txt)\n",
    "\n",
    "# save model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, '../models/random_forest_model.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "# from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# folder = StratifiedKFold(train_lbl_txt, n_folds=5, shuffle=False)\n",
    "\n",
    "# parameters = {'kernel':['linear', 'poly', 'rbf'], 'C':[64, 32, 16, 8], 'probability':[False], 'max_iter':[1000]}\n",
    "# svm_clf = SVC()\n",
    "\n",
    "# clf = GridSearchCV(svm_clf, parameters, n_jobs=-1, pre_dispatch='n_jobs', cv=folder, refit=True, verbose=5)\n",
    "# clf.fit(train_dat_scaled, train_lbl_txt)\n",
    "# clf.score(test_dat_scaled, test_lbl_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print 'best score:', clf.best_score_\n",
    "# print 'best params:', clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "clf = joblib.load('../models/random_forest_model.p')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
